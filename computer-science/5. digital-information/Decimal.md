---
title: 
description: 
resources: 
videos: 
groups:
  - "[[Digital Information]]"
tags:
  - information
---
Decimal encoding is a common method used in computing and digital communication to represent numbers, characters, and data using the decimal (base-10) numeral system. Unlike binary encoding, which uses 0s and 1s, decimal encoding uses the digits 0 through 9.

### Decimal Units

| Unit    | Value       | Number of Symbols |
| ------- | ----------- | ----------------- |
| Digit   | 1 digit     | 10                |
| Integer | n digits    | 10^n              |
| Decimal | n digits.m | 10^n * 10^-m      |

> Decimal encoding is widely used in everyday applications due to its human-friendly representation and intuitive understanding.

### Digits
- Each digit in decimal encoding represents a value from 0 to 9.
- A digit is the smallest unit of decimal encoding, analogous to a bit in binary encoding.
- In a decimal number, each digit's position determines its weight, following the powers of 10.

### Integers
- Integers are whole numbers represented using decimal encoding.
- The number of possible integers grows exponentially with the number of digits.
- For example, a two-digit decimal number can represent 100 different integers (from 00 to 99).

### Decimals
- Decimals include both integer and fractional parts, separated by a decimal point.
- The fractional part allows for precise representation of non-integer values.
- For example, the decimal 3.14 represents the integer 3 and the fraction 0.14.

### Usage
- Decimal encoding is widely used in financial calculations, where precision and accuracy are critical.
- It's also prevalent in user interfaces, text-based data representations, and everyday arithmetic operations.
